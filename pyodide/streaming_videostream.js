importScripts("https://cdn.jsdelivr.net/pyodide/v0.28.2/full/pyodide.js");

function sendPatch(patch, buffers, msg_id) {
  self.postMessage({
    type: 'patch',
    patch: patch,
    buffers: buffers
  })
}

async function startApplication() {
  console.log("Loading pyodide...");
  self.postMessage({type: 'status', msg: 'Loading pyodide'})
  self.pyodide = await loadPyodide();
  self.pyodide.globals.set("sendPatch", sendPatch);
  console.log("Loaded pyodide!");
  const data_archives = [];
  for (const archive of data_archives) {
    let zipResponse = await fetch(archive);
    let zipBinary = await zipResponse.arrayBuffer();
    self.postMessage({type: 'status', msg: `Unpacking ${archive}`})
    self.pyodide.unpackArchive(zipBinary, "zip");
  }
  await self.pyodide.loadPackage("micropip");
  self.postMessage({type: 'status', msg: `Installing environment`})
  try {
    await self.pyodide.runPythonAsync(`
      import micropip
      await micropip.install(['https://cdn.holoviz.org/panel/wheels/bokeh-3.8.0-py3-none-any.whl', 'https://cdn.holoviz.org/panel/1.8.2/dist/wheels/panel-1.8.2-py3-none-any.whl', 'pyodide-http', 'pillow', 'scikit-image']);
    `);
  } catch(e) {
    console.log(e)
    self.postMessage({
      type: 'status',
      msg: `Error while installing packages`
    });
  }
  console.log("Environment loaded!");
  self.postMessage({type: 'status', msg: 'Executing code'})
  try {
    const [docs_json, render_items, root_ids] = await self.pyodide.runPythonAsync(`\nimport asyncio\n\nfrom panel.io.pyodide import init_doc, write_doc\n\ninit_doc()\n\nfrom panel import state as _pn__state\nfrom panel.io.handlers import CELL_DISPLAY as _CELL__DISPLAY, display, get_figure as _get__figure\n\n_pn__state._cell_outputs['b8e61c5c-4f3e-4980-be22-4010a247babd'].append("""## Streaming Video: Live video streams made easy\n\nIn this example we will demonstrate how to develop a general tool to transform live image streams from the users web cam. We will be using Panels\n[\`VideoStream\`](https://panel.holoviz.org/reference/widgets/VideoStream.html) widget to record and stream the images.\n\nWe will also show how to apply *blur*, *grayscale*, *sobel* and *face recognition* models to the video stream.""")\n_pn__state._cell_outputs['5a3760b6-f8a7-46ad-8831-620a131c1512'].append("""## Imports and Settings\n\nAmong other things we will be using [numpy](https://numpy.org/), [PIL](https://pillow.readthedocs.io/en/stable/) and [scikit-image](https://scikit-image.org/) to work with the images.""")\nimport base64\nimport io\nimport time\n\nimport numpy as np\nimport param\nimport PIL\nimport skimage\n\nfrom PIL import Image, ImageFilter\nfrom skimage import data, filters\nfrom skimage.color.adapt_rgb import adapt_rgb, each_channel\nfrom skimage.draw import rectangle\nfrom skimage.exposure import rescale_intensity\nfrom skimage.feature import Cascade\n\nimport panel as pn\n\n_pn__state._cell_outputs['02b862bc-f710-456b-9a34-f1b74a76df2c'].append((pn.extension(design='material', sizing_mode="stretch_width")))\nfor _cell__out in _CELL__DISPLAY:\n    _pn__state._cell_outputs['02b862bc-f710-456b-9a34-f1b74a76df2c'].append(_cell__out)\n_CELL__DISPLAY.clear()\n_fig__out = _get__figure()\nif _fig__out:\n    _pn__state._cell_outputs['02b862bc-f710-456b-9a34-f1b74a76df2c'].append(_fig__out)\n\n_pn__state._cell_outputs['43e68289-6c33-435f-b279-cf89327266ac'].append("""We define the *height* and *width* of the images to transform. Smaller is faster.\nWe also define the *timeout*, i.e. how often the videostream takes and streams a new image.""")\nHEIGHT = 500 # pixels\nWIDTH = 500 # pixels\nTIMEOUT = 500\n_pn__state._cell_outputs['bb8b1066-c799-4d6a-94d0-361dd3cc6007'].append("""## Base Image Models\n\nWe will need to define some *base image models* components. The base models are custom Panel components that inherit from Panels [\`Viewer\`](https://panel.holoviz.org/how_to/custom_components/custom_viewer.html) class.\n\nThe *base models* makes it easy to later turn *image to image* algorithms into interactive UIs like the \`FaceDetectionModel\` shown in the image just below.\n\n<img src=\\"../assets/VideoStreamInterfaceFaceDetectionViewer.jpg\\" style=\\"margin-left: auto; margin-right: auto; display: block;\\"></img>\n\nPlease note we restrict our selves to working with \`.jpg\` images. The \`VideoStream\` widget also support \`.png\` images. But \`.png\` images are much bigger and slower to work with.""")\nclass ImageModel(pn.viewable.Viewer):\n    """Base class for image models."""\n\n    def __init__(self, **params):\n        super().__init__(**params)\n\n        with param.edit_constant(self):\n            self.name = self.__class__.name.replace("Model", "")\n        self.view = self.create_view()\n\n    def __panel__(self):\n        return self.view\n\n    def apply(self, image: str, height: int = HEIGHT, width: int = WIDTH) -> str:\n        """Transforms a base64 encoded jpg image to a base64 encoded jpg BytesIO object"""\n        raise NotImplementedError()\n\n    def create_view(self):\n        """Creates a view of the parameters of the transform to enable the user to configure them"""\n        return pn.Param(self, name=self.name)\n\n    def transform(self, image):\n        """Transforms the image"""\n        raise NotImplementedError()\n_pn__state._cell_outputs['be8256eb-f2bb-4f24-84dc-8b6b47d64958'].append("""Lets define a base model for working with **\`PIL\`** images""")\nclass PILImageModel(ImageModel):\n    """Base class for PIL image models"""\n\n    @staticmethod\n    def to_pil_img(value: str, height=HEIGHT, width=WIDTH):\n        """Converts a base64 jpeg image string to a PIL.Image"""\n        encoded_data = value.split(",")[1]\n        base64_decoded = base64.b64decode(encoded_data)\n        image = Image.open(io.BytesIO(base64_decoded))\n        image.draft("RGB", (height, width))\n        return image\n\n    @staticmethod\n    def from_pil_img(image: Image):\n        """Converts a PIL.Image to a base64 encoded JPG BytesIO object"""\n        buff = io.BytesIO()\n        image.save(buff, format="JPEG")\n        return buff\n\n    def apply(self, image: str, height: int = HEIGHT, width: int = WIDTH) -> io.BytesIO:\n        pil_img = self.to_pil_img(image, height=height, width=width)\n\n        transformed_image = self.transform(pil_img)\n\n        return self.from_pil_img(transformed_image)\n\n    def transform(self, image: PIL.Image) -> PIL.Image:\n        """Transforms the PIL.Image image"""\n        raise NotImplementedError()\n_pn__state._cell_outputs['63cff6f1-f652-4413-b230-b4602e115560'].append("""Lets define a base model for working with **\`Numpy\`** images.""")\nclass NumpyImageModel(ImageModel):\n    """Base class for np.ndarray image models"""\n\n    @staticmethod\n    def to_np_ndarray(image: str, height=HEIGHT, width=WIDTH) -> np.ndarray:\n        """Converts a base64 encoded jpeg string to a np.ndarray"""\n        pil_img = PILImageModel.to_pil_img(image, height=height, width=width)\n        return np.array(pil_img)\n\n    @staticmethod\n    def from_np_ndarray(image: np.ndarray) -> io.BytesIO:\n        """Converts np.ndarray jpeg image to a jpeg BytesIO instance"""\n        if image.dtype == np.dtype("float64"):\n            image = (image * 255).astype(np.uint8)\n        pil_img = PIL.Image.fromarray(image)\n        return PILImageModel.from_pil_img(pil_img)\n\n    def apply(self, image: str, height: int = HEIGHT, width: int = WIDTH) -> io.BytesIO:\n        np_array = self.to_np_ndarray(image, height=height, width=width)\n\n        transformed_image = self.transform(np_array)\n\n        return self.from_np_ndarray(transformed_image)\n\n    def transform(self, image: np.ndarray) -> np.ndarray:\n        """Transforms the np.array image"""\n        raise NotImplementedError()\n_pn__state._cell_outputs['77bf1e99-701e-4dad-bd8a-faba4bdd446a'].append("""## Timer\n\nLets define a timer component to visualize the stats of the live videostream and the image transformations\n\n<img src=\\"../assets/VideoStreamInterfaceTimer.jpg\\" style=\\"margin-left: auto; margin-right: auto; display: block;\\"></img>""")\nclass Timer(pn.viewable.Viewer):\n    """Helper Component used to show duration trends"""\n\n    _trends = param.Dict()\n\n    def __init__(self, **params):\n        super().__init__()\n\n        self.last_updates = {}\n        self._trends = {}\n\n        self._layout = pn.Row(**params)\n\n    def time_it(self, name, func, *args, **kwargs):\n        """Measures the duration of the execution of the func function and reports it under the\n        name specified"""\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        duration = round(end - start, 2)\n        self._report(name=name, duration=duration)\n        return result\n\n    def inc_it(self, name):\n        """Measures the duration since the last time \`inc_it\` was called and reports it under the\n        specified name"""\n        start = self.last_updates.get(name, time.time())\n        end = time.time()\n        duration = round(end - start, 2)\n        self._report(name=name, duration=duration)\n        self.last_updates[name] = end\n\n    def _report(self, name, duration):\n        if not name in self._trends:\n            self._trends[name] = pn.indicators.Trend(\n                name=name,\n                data={"x": [1], "y": [duration]},\n                height=100,\n                width=150,\n                sizing_mode="fixed",\n            )\n            self.param.trigger("_trends")\n        else:\n            trend = self._trends[name]\n            next_x = max(trend.data["x"]) + 1\n            trend.stream({"x": [next_x], "y": [duration]}, rollover=10)\n\n    @param.depends("_trends")\n    def _panel(self):\n        self._layout[:] = list(self._trends.values())\n        return self._layout\n\n    def __panel__(self):\n        return pn.panel(self._panel)\n_pn__state._cell_outputs['44851482-f580-49a7-ac53-2132d3f29bc0'].append("""## VideoStreamInterface\n\nThe \`VideoStreamInterface\` will be putting things together in a nice UI.\n\n<img src=\\"../assets/VideoStreamInterfaceSobel.jpg\\" style=\\"margin-left: auto; margin-right: auto; display: block; max-height:500px;\\"></img>\n\nLets define a helper function first""")\ndef to_instance(value, **params):\n    """Converts the value to an instance\n\n    Args:\n        value: A param.Parameterized class or instance\n\n    Returns:\n        An instance of the param.Parameterized class\n    """\n    if isinstance(value, param.Parameterized):\n        value.param.update(**params)\n        return value\n    return value(**params)\n_pn__state._cell_outputs['d74ba396-7972-4711-8ff5-b4d27b31120c'].append("""The \`VideoStreamInterface\` will take a list of \`ImageModel\`s. The user can the select and apply the models to the images from the \`VideoStream\`.""")\nclass VideoStreamInterface(pn.viewable.Viewer):\n    """An easy to use interface for a VideoStream and a set of transforms"""\n\n    video_stream = param.ClassSelector(\n        class_=pn.widgets.VideoStream, constant=True, doc="The source VideoStream"\n    )\n\n    height = param.Integer(\n        default=HEIGHT,\n        bounds=(10, 2000),\n        step=10,\n        doc="""The height of the image converted and shown""",\n    )\n    width = param.Integer(\n        default=WIDTH,\n        bounds=(10, 2000),\n        step=10,\n        doc="""The width of the image converted and shown""",\n    )\n\n    model = param.Selector(doc="The currently selected model")\n\n    def __init__(\n        self,\n        models,\n        timeout=TIMEOUT,\n        paused=False,\n        **params,\n    ):\n        super().__init__(\n            video_stream=pn.widgets.VideoStream(\n                name="Video Stream",\n                timeout=timeout,\n                paused=paused,\n                height=0,\n                width=0,\n                visible=False,\n                format="jpeg",\n            ),\n            **params,\n        )\n        self.image = pn.pane.JPG(\n            height=self.height, width=self.width, sizing_mode="fixed"\n        )\n        self._updating = False\n        models = [to_instance(model) for model in models]\n        self.param.model.objects = models\n        self.model = models[0]\n        self.timer = Timer(sizing_mode="stretch_width")\n        self.settings = self._create_settings()\n        self._panel = self._create_panel()\n\n    def _create_settings(self):\n        return pn.Column(\n            pn.Param(\n                self.video_stream,\n                parameters=["timeout", "paused"],\n                widgets={\n                    "timeout": {\n                        "widget_type": pn.widgets.IntSlider,\n                        "start": 10,\n                        "end": 2000,\n                        "step": 10,\n                    }\n                },\n            ),\n            self.timer,\n            pn.Param(self, parameters=["height", "width"], name="Image"),\n            pn.Param(\n                self,\n                parameters=["model"],\n                expand_button=False,\n                expand=False,\n                widgets={\n                    "model": {\n                        "widget_type": pn.widgets.RadioButtonGroup,\n                        "orientation": "vertical",\n                        "button_type": "primary",\n                        "button_style": "outline"\n                    }\n                },\n                name="Model",\n            ),\n            self._get_transform,\n        )\n\n    def _create_panel(self):\n        return pn.Row(\n            self.video_stream,\n            pn.layout.HSpacer(),\n            self.image,\n            pn.layout.HSpacer(),\n            sizing_mode="stretch_width",\n            align="center",\n        )\n\n    @param.depends("height", "width", watch=True)\n    def _update_height_width(self):\n        self.image.height = self.height\n        self.image.width = self.width\n\n    @param.depends("model")\n    def _get_transform(self):\n        # Hack: returning self.transform stops working after browsing the transforms for a while\n        return self.model.view\n\n    def __panel__(self):\n        return self._panel\n\n    @param.depends("video_stream.value", watch=True)\n    def _handle_stream(self):\n        if self._updating:\n            return\n\n        self._updating = True\n        if self.model and self.video_stream.value:\n            value = self.video_stream.value\n            try:\n                image = self.timer.time_it(\n                    name="Model",\n                    func=self.model.apply,\n                    image=value,\n                    height=self.height,\n                    width=self.width,\n                )\n                self.image.object = image\n            except PIL.UnidentifiedImageError:\n                print("unidentified image")\n\n            self.timer.inc_it("Last Update")\n        self._updating = False\n_pn__state._cell_outputs['9afa4c9b-a541-4a77-bb38-68d85f0a82d4'].append("""## Custom Image Models\n\nWe will now make specific image to image algorithms interactive.\n\nLet us start with the [Gaussian Blur](https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html#PIL.ImageFilter.GaussianBlur) algorithm.""")\nclass GaussianBlurModel(PILImageModel):\n    """Gaussian Blur Model\n\n    https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html#PIL.ImageFilter.GaussianBlur\n    """\n\n    radius = param.Integer(default=2, bounds=(0, 10))\n\n    def transform(self, image: Image):\n        return image.filter(ImageFilter.GaussianBlur(radius=self.radius))\n_pn__state._cell_outputs['42d93547-bfce-4deb-a7e5-387cab9fac8e'].append("""Lets implement a [Grayscale](https://scikit-image.org/docs/0.15.x/auto_examples/color_exposure/plot_rgb_to_gray.html) algorithm.""")\nclass GrayscaleModel(NumpyImageModel):\n    """GrayScale Model\n\n    https://scikit-image.org/docs/0.15.x/auto_examples/color_exposure/plot_rgb_to_gray.html\n    """\n\n    def transform(self, image: np.ndarray):\n        grayscale = skimage.color.rgb2gray(image[:, :, :3])\n        return skimage.color.gray2rgb(grayscale)\n_pn__state._cell_outputs['b3461593-e63d-424a-b402-cbd6fcc75ffe'].append("""Lets implement the [Sobel](https://scikit-image.org/docs/0.15.x/auto_examples/color_exposure/plot_adapt_rgb.html) algorithm.""")\nclass SobelModel(NumpyImageModel):\n    """Sobel Model\n\n    https://scikit-image.org/docs/0.15.x/auto_examples/color_exposure/plot_adapt_rgb.html\n    """\n    def transform(self, image):\n\n\n        @adapt_rgb(each_channel)\n        def sobel_each(image):\n            return filters.sobel(image)\n\n        return rescale_intensity(1 - sobel_each(image))\n_pn__state._cell_outputs['cadae0c9-0191-4ca4-a770-c3be0b0987b7'].append("""Lets implement the [face detection model](https://scikit-image.org/docs/0.15.x/auto_examples/applications/plot_face_detection.html) of scikit-image.""")\n@pn.cache()\ndef get_detector():\n    """Returns the Cascade detector"""\n    trained_file = data.lbp_frontal_face_cascade_filename()\n    return Cascade(trained_file)\nclass FaceDetectionModel(NumpyImageModel):\n    """Face detection using a cascade classifier.\n\n    https://scikit-image.org/docs/0.15.x/auto_examples/applications/plot_face_detection.html\n    """\n\n    scale_factor = param.Number(default=1.4, bounds=(1.0, 2.0), step=0.1)\n    step_ratio = param.Integer(default=1, bounds=(1, 10))\n    size_x = param.Range(default=(60, 322), bounds=(10, 500))\n    size_y = param.Range(default=(60, 322), bounds=(10, 500))\n\n    def transform(self, image):\n        detector = get_detector()\n        detected = detector.detect_multi_scale(\n            img=image,\n            scale_factor=self.scale_factor,\n            step_ratio=self.step_ratio,\n            min_size=(self.size_x[0], self.size_y[0]),\n            max_size=(self.size_x[1], self.size_y[1]),\n        )\n\n        for patch in detected:\n            rrr, ccc = rectangle(\n                start=(patch["r"], patch["c"]),\n                extent=(patch["height"], patch["width"]),\n                shape=image.shape[:2],\n            )\n            image[rrr, ccc, 0] = 200\n\n        return image\n_pn__state._cell_outputs['253e15c1-0097-456f-b979-fbf42de8d270'].append("""Please note that these models are just examples. You can also implement your own models using Scikit-Image, Pytorch, Tensorflow etc and use the \`VideoStreamInterface\` to work interactively with them.""")\n_pn__state._cell_outputs['1e6ffe9c-7885-4cb8-9793-97c00862e8ba'].append("""## Its alive!\n\nLets define an instance of the \`VideoStreamInterface\`""")\ncomponent = VideoStreamInterface(\n    models=[\n        GaussianBlurModel,\n        GrayscaleModel,\n        SobelModel,\n        FaceDetectionModel,\n    ]\n)\n_pn__state._cell_outputs['47d8118f-77b0-4edf-a733-2b9ba877e25e'].append((pn.Row(pn.Row(component.settings, max_width=400), component)))\nfor _cell__out in _CELL__DISPLAY:\n    _pn__state._cell_outputs['47d8118f-77b0-4edf-a733-2b9ba877e25e'].append(_cell__out)\n_CELL__DISPLAY.clear()\n_fig__out = _get__figure()\nif _fig__out:\n    _pn__state._cell_outputs['47d8118f-77b0-4edf-a733-2b9ba877e25e'].append(_fig__out)\n\n_pn__state._cell_outputs['651ea40a-4d91-4a17-8761-c8b5cc446197'].append("""## Wrap it in a template\n\nWhat makes Panel unique is that our components work very well in both the notebook and as standalone data apps.\n\nWe can wrap the component in the nicely styled [\`FastListTemplate\`](https://panel.holoviz.org/reference/templates/FastListTemplate.html) to make it *ready for production*.""")\npn.template.FastListTemplate(\n    site="Panel",\n    title="VideoStream Interface",\n    sidebar=[component.settings],\n    main=[component],\n).servable(); # We add ; to not show the template in the notebook as it does not display well.\n_pn__state._cell_outputs['9b00da3e-87fd-4e3e-bc81-e8bf827e2818'].append("""## Serve it as a server side app""")\n_pn__state._cell_outputs['7191262b-f610-494f-a797-9186a496c633'].append("""It is now possible to serve the live app via the command \`panel serve streaming_videostream.ipynb\`. The app is the available at http://localhost:5006/streaming_videostream.\n\n<img src=\\"../assets/VideoStreamInterface.jpg\\" style=\\"margin-left: auto; margin-right: auto; display: block; max-height:500px\\"></img>""")\n_pn__state._cell_outputs['01fb5b28-6e49-4aaa-a0ca-2eaa301106c2'].append("""## Serve it as a client side app\n\nYou can also [\`panel convert\`](https://panel.holoviz.org/how_to/wasm/convert.html) this app to web assembly for even better performance.\n\nFirst you will need to create a \`requirements.txt\` file with the following content\n\n\`\`\`bash\npanel\nnumpy\nscikit-image\n\`\`\`\n\nThen you can\n\n- Run \`panel convert streaming_videostream.ipynb --to pyodide-worker --out pyodide --requirements requirements.txt\`\n- Run \`python3 -m http.server\` to start a web server locally\n- Open http://localhost:8000/pyodide/streaming_videostream.html to try out the app.""")\n\nawait write_doc()`)
    self.postMessage({
      type: 'render',
      docs_json: docs_json,
      render_items: render_items,
      root_ids: root_ids
    })
  } catch(e) {
    const traceback = `${e}`
    const tblines = traceback.split('\n')
    self.postMessage({
      type: 'status',
      msg: tblines[tblines.length-2]
    });
    throw e
  }
}

self.onmessage = async (event) => {
  const msg = event.data
  if (msg.type === 'rendered') {
    self.pyodide.runPythonAsync(`
    from panel.io.state import state
    from panel.io.pyodide import _link_docs_worker

    _link_docs_worker(state.curdoc, sendPatch, setter='js')
    `)
  } else if (msg.type === 'patch') {
    self.pyodide.globals.set('patch', msg.patch)
    self.pyodide.runPythonAsync(`
    from panel.io.pyodide import _convert_json_patch
    state.curdoc.apply_json_patch(_convert_json_patch(patch), setter='js')
    `)
    self.postMessage({type: 'idle'})
  } else if (msg.type === 'location') {
    self.pyodide.globals.set('location', msg.location)
    self.pyodide.runPythonAsync(`
    import json
    from panel.io.state import state
    from panel.util import edit_readonly
    if state.location:
        loc_data = json.loads(location)
        with edit_readonly(state.location):
            state.location.param.update({
                k: v for k, v in loc_data.items() if k in state.location.param
            })
    `)
  }
}

startApplication()